{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT For Me: A Langchain Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert your values here\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "DEPLOYMENT_NAME = os.environ[\"DEPLOYMENT_NAME\"]\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature \n",
    "Temerature in LLMs is a parameter to define the randomness of the generated response. Therefore, the higher the temperature, the more creative the response appears and the lower the response, the more precise the model's generations appear to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(deployment_name=DEPLOYMENT_NAME, \n",
    "                  temperature=0.2\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D768AF10>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Satya Nadella (2014-present)\\n2. Steve Ballmer (2000-2014)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"List the last two Microsoft CEOs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"List the last two {company} CEOS\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptVavlue = prompt.format(company=\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7274610>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Tim Cook (2011-present)\\n2. Steve Jobs (1997-2011)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(promptVavlue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you have mulitple input values to expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D71EA390>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Sundar Pichai (2015-present)\\n2. Larry Page (2011-2015)\\n3. Eric Schmidt (2001-2011)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt_template = \"List the last {count} {company} CEOs\"\n",
    "\n",
    "new_prompt = PromptTemplate.from_template(new_prompt_template)\n",
    "# also same as \n",
    "# new_prompt = PromptTemplate(template=new_prompt_template,\n",
    "#                       input_variables=[\"count\", \"company\"])\n",
    "\n",
    "new_prompt_value = new_prompt.format(count=\"three\", company=\"Google\")\n",
    "llm.predict(new_prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "There are mulitple text splitters available in Langchain. You can chose to split by character, code, or tokens. Langchain also provides a Markdown header splitter.\n",
    "\n",
    "For this exmaple, we'll use the Recursive Character Text Splitter. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlapping between the chunks also helps to ensure that important features aren't missed at the boundaries of the chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Analyze and predict threats to your secrets: Ensure that all secret accesses are analyzed and' metadata={}\n",
      "page_content='are analyzed and systems are in place that can detect a deviation from normal usage and alert' metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"internal_doc.txt\", encoding=\"utf8\") as file:\n",
    "    internal_doc = file.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100, \n",
    "    chunk_overlap= 20\n",
    ")\n",
    "\n",
    "split_texts = text_splitter.create_documents([internal_doc])\n",
    "print(split_texts[50])\n",
    "print(split_texts[51])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "You can upload to using Microsoft Services, specifically Azure Cognitive Search. It also serves as a vector store that we can later use a retriever for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "#pip install azure-search-documents\n",
    "#pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_COGNITIVE_SEARCH_URL = os.environ[\"AZURE_COGNITIVE_SEARCH_URL\"]\n",
    "AZURE_COGNITIVE_SEARCH_PASSWORD = os.environ[\"AZURE_COGNITIVE_SEARCH_PASSWORD\"]\n",
    "AZURE_COGNITIVE_SEARCH_INDEX_NAME = os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "import uuid\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=200)\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_COGNITIVE_SEARCH_URL,\n",
    "    azure_search_key=AZURE_COGNITIVE_SEARCH_PASSWORD,\n",
    "    index_name=AZURE_COGNITIVE_SEARCH_INDEX_NAME,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "docs_to_embed = []\n",
    "for doc in split_texts:\n",
    "    idx_fields = {\n",
    "    \"id\": str(uuid.uuid1()),\n",
    "    \"content\": doc.page_content, \n",
    "    }  \n",
    "    docs_to_embed.append(idx_fields)\n",
    "search_client = SearchClient(AZURE_COGNITIVE_SEARCH_URL, AZURE_COGNITIVE_SEARCH_INDEX_NAME, AzureKeyCredential(AZURE_COGNITIVE_SEARCH_PASSWORD))\n",
    "try:\n",
    "    #upload to Azure Cognitive Store for embedding\n",
    "    search_client.upload_documents(docs_to_embed)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList the last three Netflix CEOs\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D71388D0>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Reed Hastings (2002-present)\\n2. Ted Sarandos (co-CEO, 2020-present)\\n3. Greg Peters (co-CEO, 2020-present)'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "ceo_chain = LLMChain(llm=llm, prompt=new_prompt, output_key=\"ceos_list\", verbose=True)\n",
    "ceo_chain.run({\n",
    "    \"count\": \"three\",\n",
    "    \"company\": \"Netflix\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got to search function\n",
      "got here \n",
      "response is <urllib3.response.HTTPResponse object at 0x000001A6D983F6A0>\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D9315650>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the context of cybersecurity, secrets refer to sensitive information that is used to authenticate or authorize access to systems, applications, or data. Examples of secrets include passwords, cryptographic keys, tokens, and certificates. Adversaries may attempt to steal secrets in order to gain unauthorized access to systems or data. They may use various techniques such as phishing, social engineering, or exploiting vulnerabilities in software or systems to obtain secrets. It is important to protect secrets by using strong authentication mechanisms, encrypting sensitive data, and implementing access controls to limit who can access secrets. Additionally, it is important to regularly monitor and audit access to secrets to detect and respond to any unauthorized access attempts.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "\n",
    "retriever = AzureCognitiveSearchRetriever(content_key=\"content\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(\"tell me about secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequential Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList the last two Apple CEOs\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7151490>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven a list of CEOs, return their age of when they became CEOs.\n",
      "CEOs: 1. Tim Cook (2011-present)\n",
      "2. Steve Jobs (1997-2011)\n",
      "\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7218790>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count': 'two',\n",
       " 'company': 'Apple',\n",
       " 'answer': '3. Jeff Bezos (1994-present)\\n4. Mark Zuckerberg (2004-present)\\n5. Sundar Pichai (2015-present)\\n\\n1. Tim Cook became CEO in 2011 at the age of 50.\\n2. Steve Jobs became CEO in 1997 at the age of 42.\\n3. Jeff Bezos became CEO in 1994 at the age of 30.\\n4. Mark Zuckerberg became CEO in 2004 at the age of 19.\\n5. Sundar Pichai became CEO in 2015 at the age of 43.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "second_template = \"\"\"Given a list of CEOs, return their age of when they became CEOs.\n",
    "CEOs: {ceos_list}\n",
    "\"\"\"\n",
    "ceo_age_prompt = PromptTemplate.from_template(second_template)\n",
    "ceo_age_chain = LLMChain(llm=llm, prompt=ceo_age_prompt, output_key=\"answer\", verbose=True)\n",
    "final_chain = SequentialChain(chains=[ceo_chain, ceo_age_chain],\n",
    "                                    input_variables = [\"count\", \"company\"],\n",
    "                                    verbose=True)\n",
    "\n",
    "final_chain({\n",
    "    \"count\": \"two\",\n",
    "    \"company\": \"Apple\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass  `serpapi_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_agent\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n\u001b[1;32m----> 5\u001b[0m tools \u001b[39m=\u001b[39m load_tools([\u001b[39m\"\u001b[39;49m\u001b[39mserpapi\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mllm-math\u001b[39;49m\u001b[39m\"\u001b[39;49m], llm\u001b[39m=\u001b[39;49mllm)\n\u001b[0;32m      6\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools\u001b[39m=\u001b[39mtools, llm\u001b[39m=\u001b[39mllm, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m agent\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mWhat new Bing chat feature did Microsoft announce this week?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\t-yonasg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\agents\\load_tools.py:452\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m     _get_tool_func, extra_keys \u001b[39m=\u001b[39m _EXTRA_OPTIONAL_TOOLS[name]\n\u001b[0;32m    451\u001b[0m     sub_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m extra_keys \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs}\n\u001b[1;32m--> 452\u001b[0m     tool \u001b[39m=\u001b[39m _get_tool_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msub_kwargs)\n\u001b[0;32m    453\u001b[0m     tools\u001b[39m.\u001b[39mappend(tool)\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\t-yonasg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\agents\\load_tools.py:229\u001b[0m, in \u001b[0;36m_get_serpapi\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_serpapi\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseTool:\n\u001b[0;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m Tool(\n\u001b[0;32m    227\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSearch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    228\u001b[0m         description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA search engine. Useful for when you need to answer questions about current events. Input should be a search query.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> 229\u001b[0m         func\u001b[39m=\u001b[39mSerpAPIWrapper(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mrun,\n\u001b[0;32m    230\u001b[0m         coroutine\u001b[39m=\u001b[39mSerpAPIWrapper(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39marun,\n\u001b[0;32m    231\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\t-yonasg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass  `serpapi_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"What new Bing chat feature did Microsoft announce this week?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
