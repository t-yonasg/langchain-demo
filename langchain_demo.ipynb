{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT For Me: A Langchain Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert your values here\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "DEPLOYMENT_NAME = os.environ[\"DEPLOYMENT_NAME\"]\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature \n",
    "Temerature in LLMs is a parameter to define the randomness of the generated response. Therefore, the higher the temperature, the more creative the response appears and the lower the response, the more precise the model's generations appear to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(deployment_name=DEPLOYMENT_NAME, \n",
    "                  temperature=0.2\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D768AF10>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Satya Nadella (2014-present)\\n2. Steve Ballmer (2000-2014)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"List the last two Microsoft CEOs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"List the last two {company} CEOS\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptVavlue = prompt.format(company=\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7274610>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Tim Cook (2011-present)\\n2. Steve Jobs (1997-2011)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(promptVavlue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you have mulitple input values to expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D71EA390>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Sundar Pichai (2015-present)\\n2. Larry Page (2011-2015)\\n3. Eric Schmidt (2001-2011)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt_template = \"List the last {count} {company} CEOs\"\n",
    "\n",
    "new_prompt = PromptTemplate.from_template(new_prompt_template)\n",
    "# also same as \n",
    "# new_prompt = PromptTemplate(template=new_prompt_template,\n",
    "#                       input_variables=[\"count\", \"company\"])\n",
    "\n",
    "new_prompt_value = new_prompt.format(count=\"three\", company=\"Google\")\n",
    "llm.predict(new_prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "There are mulitple text splitters available in Langchain. You can chose to split by character, code, or tokens. Langchain also provides a Markdown header splitter.\n",
    "\n",
    "For this exmaple, we'll use the Recursive Character Text Splitter. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlapping between the chunks also helps to ensure that important features aren't missed at the boundaries of the chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='scene that absolutely broke the theater I was in.' metadata={}\n",
      "page_content='michael m\\nSUPER REVIEWER\\nApr 18, 2022' metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"movie_review.txt\", encoding=\"utf8\") as file:\n",
    "    movie_reviews = file.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100, \n",
    "    chunk_overlap= 20\n",
    ")\n",
    "\n",
    "split_texts = text_splitter.create_documents([movie_reviews])\n",
    "print(split_texts[50])\n",
    "print(split_texts[51])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "You can upload to using Microsoft Services, specifically Azure Cognitive Search. It also serves as a vector store that we can later use a retriever for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "#pip install azure-search-documents\n",
    "#pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_COGNITIVE_SEARCH_URL = os.environ[\"AZURE_COGNITIVE_SEARCH_URL\"]\n",
    "AZURE_COGNITIVE_SEARCH_PASSWORD = os.environ[\"AZURE_COGNITIVE_SEARCH_PASSWORD\"]\n",
    "AZURE_COGNITIVE_SEARCH_INDEX_NAME = os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "import uuid\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=200)\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_COGNITIVE_SEARCH_URL,\n",
    "    azure_search_key=AZURE_COGNITIVE_SEARCH_PASSWORD,\n",
    "    index_name=AZURE_COGNITIVE_SEARCH_INDEX_NAME,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "docs_to_embed = []\n",
    "for doc in split_texts:\n",
    "    idx_fields = {\n",
    "    \"id\": str(uuid.uuid1()),\n",
    "    \"content\": doc.page_content, \n",
    "    }  \n",
    "    docs_to_embed.append(idx_fields)\n",
    "search_client = SearchClient(AZURE_COGNITIVE_SEARCH_URL, AZURE_COGNITIVE_SEARCH_INDEX_NAME, AzureKeyCredential(AZURE_COGNITIVE_SEARCH_PASSWORD))\n",
    "try:\n",
    "    #upload to Azure Cognitive Store for embedding\n",
    "    search_client.upload_documents(docs_to_embed)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList the last three Netflix CEOs\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D71388D0>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Reed Hastings (2002-present)\\n2. Ted Sarandos (co-CEO, 2020-present)\\n3. Greg Peters (co-CEO, 2020-present)'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "ceo_chain = LLMChain(llm=llm, prompt=new_prompt, output_key=\"ceos_list\", verbose=True)\n",
    "ceo_chain.run({\n",
    "    \"count\": \"three\",\n",
    "    \"company\": \"Netflix\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got to search function\n",
      "got here \n",
      "response is <urllib3.response.HTTPResponse object at 0x000001A6D9B24E80>\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D93F4DD0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The reviews for the Sonic movie are mixed. Some critics praise the movie for being a fun and enjoyable kids movie, with great performances from Jim Carrey and Ben Schwartz as Sonic. They also note that the movie stays true to the Sonic video game franchise and includes elements that fans will appreciate. However, other critics criticize the movie for being a rehash of the first movie, lacking originality and relying too heavily on recycled elements. Some also note that the movie feels like it's aimed primarily at children and doesn't work on multiple levels. Overall, the reviews suggest that the Sonic movie is a fun and entertaining movie for kids, but may not be as enjoyable for adults or fans of the video game franchise.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "\n",
    "retriever = AzureCognitiveSearchRetriever(content_key=\"content\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(\"Give a summary of the reviews for the sonic movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequential Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList the last two Apple CEOs\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7151490>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven a list of CEOs, return their age of when they became CEOs.\n",
      "CEOs: 1. Tim Cook (2011-present)\n",
      "2. Steve Jobs (1997-2011)\n",
      "\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D7218790>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count': 'two',\n",
       " 'company': 'Apple',\n",
       " 'answer': '3. Jeff Bezos (1994-present)\\n4. Mark Zuckerberg (2004-present)\\n5. Sundar Pichai (2015-present)\\n\\n1. Tim Cook became CEO in 2011 at the age of 50.\\n2. Steve Jobs became CEO in 1997 at the age of 42.\\n3. Jeff Bezos became CEO in 1994 at the age of 30.\\n4. Mark Zuckerberg became CEO in 2004 at the age of 19.\\n5. Sundar Pichai became CEO in 2015 at the age of 43.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "second_template = \"\"\"Given a list of CEOs, return their age of when they became CEOs.\n",
    "CEOs: {ceos_list}\n",
    "\"\"\"\n",
    "ceo_age_prompt = PromptTemplate.from_template(second_template)\n",
    "ceo_age_chain = LLMChain(llm=llm, prompt=ceo_age_prompt, output_key=\"answer\", verbose=True)\n",
    "final_chain = SequentialChain(chains=[ceo_chain, ceo_age_chain],\n",
    "                                    input_variables = [\"count\", \"company\"],\n",
    "                                    verbose=True)\n",
    "\n",
    "final_chain({\n",
    "    \"count\": \"two\",\n",
    "    \"company\": \"Apple\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "<openai.openai_response.OpenAIResponse object at 0x000001A6D784F290>\n",
      "\u001b[32;1m\u001b[1;3mI need to search for recent news about Microsoft and Bing.\n",
      "Action: Search\n",
      "Action Input: \"Microsoft Bing chat feature announcement\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mToday at Microsoft Inspire, we're excited to unveil the next steps in our journey: First, we're significantly expanding Bing to reach new ...\u001b[0m\n",
      "Thought:<openai.openai_response.OpenAIResponse object at 0x000001A6D7438590>\n",
      "\u001b[32;1m\u001b[1;3mI found the article, but it doesn't mention the name of the new chat feature.\n",
      "Action: Search\n",
      "Action Input: \"Microsoft Bing new chat feature name\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMicrosoft revealed a new version of Bing in early February that boasts a noteworthy capability: integration with ChatGPT. Microsoft claims that the chat feature of the new Bing is now even more powerful than ChatGPT itself, thanks to the integration with a next-generation version of OpenAI's large language model.\u001b[0m\n",
      "Thought:<openai.openai_response.OpenAIResponse object at 0x000001A6D743B650>\n",
      "\u001b[32;1m\u001b[1;3mI found the name of the new chat feature, but I need to know more about ChatGPT and OpenAI.\n",
      "Action: Search\n",
      "Action Input: \"ChatGPT and OpenAI\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mChatGPT is an artificial intelligence chatbot developed by OpenAI and launched on November 30, 2022. It is notable for enabling users to refine and steer a conversation towards a desired length, format, style, level of detail, and language used.\u001b[0m\n",
      "Thought:<openai.openai_response.OpenAIResponse object at 0x000001A6D784D950>\n",
      "\u001b[32;1m\u001b[1;3mI have all the information I need.\n",
      "Final Answer: Microsoft announced a new version of Bing that integrates with ChatGPT, an AI chatbot developed by OpenAI. The new chat feature is more powerful than ChatGPT itself and does not have a specific name mentioned in the articles.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Microsoft announced a new version of Bing that integrates with ChatGPT, an AI chatbot developed by OpenAI. The new chat feature is more powerful than ChatGPT itself and does not have a specific name mentioned in the articles.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"What new Bing chat feature did Microsoft announce this week?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
